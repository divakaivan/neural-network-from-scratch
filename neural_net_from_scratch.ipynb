{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "S2h3va3vNIaG"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
      ],
      "metadata": {
        "id": "ZmqW7kkKae7S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f247f891-d19d-4be1-a22f-422985f2b5ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-11 11:50:07--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "\rnames.txt             0%[                    ]       0  --.-KB/s               \rnames.txt           100%[===================>] 222.80K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-03-11 11:50:07 (13.1 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "ynNAdEfMawQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'names.txt'\n",
        "words = open(file_path, 'r').read().splitlines()\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "char_to_idx = {char: idx+1 for idx, char in enumerate(chars)}\n",
        "char_to_idx['.'] = 0\n",
        "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
        "vocab_size = len(idx_to_char)\n",
        "\n",
        "print(f'{idx_to_char=}\\n{vocab_size=}')"
      ],
      "metadata": {
        "id": "1C9CxnP3NVqm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d028ffba-552c-4621-bcfb-4acd714c4179"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "idx_to_char={1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "vocab_size=27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dataloader"
      ],
      "metadata": {
        "id": "IX6v-4oGazWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(words, char_to_idx, context_size=3):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * context_size\n",
        "    for char in w + '.':\n",
        "      idx = char_to_idx[char]\n",
        "      X.append(context)\n",
        "      Y.append(idx)\n",
        "      context = context[1:] + [idx]\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(f'{X.shape=}, {Y.shape=}')\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "mO4Z9s96ab3N"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 3\n",
        "train_size = 0.80\n",
        "split = int(train_size * len(words))\n",
        "X_train, y_train = build_dataset(words[:split], char_to_idx, context_size=context_size)\n",
        "X_valid, y_valid = build_dataset(words[split:], char_to_idx, context_size=context_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KTYd2aqbfAd",
        "outputId": "251a11b4-c48d-417f-ff87-ad0add28dc9b"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape=torch.Size([194078, 3]), Y.shape=torch.Size([194078])\n",
            "X.shape=torch.Size([34068, 3]), Y.shape=torch.Size([34068])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create model"
      ],
      "metadata": {
        "id": "fKwbe46xer9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_size = 10\n",
        "hidden_size = 200\n",
        "\n",
        "generator = torch.Generator().manual_seed(999)\n",
        "char_emb = torch.randn((vocab_size, emb_size), generator=generator)\n",
        "# Layer 1\n",
        "W1 = torch.randn((emb_size * context_size, hidden_size), generator=generator) * (5/3)/((emb_size * context_size)**0.5)\n",
        "b1 = torch.randn(hidden_size, generator=generator) * 0.1\n",
        "# Layer 2\n",
        "W2 = torch.randn((hidden_size, hidden_size), generator=generator) * 0.1\n",
        "b2 = torch.randn(hidden_size, generator=generator) * 0.1\n",
        "# Layer 3\n",
        "# W3 = torch.randn((hidden_size, vocab_size), generator=generator) * 0.1\n",
        "# b3 = torch.randn(vocab_size, generator=generator) * 0.1\n",
        "# Batch Norm params\n",
        "batch_norm_scale_1 = torch.randn((1, hidden_size)) * 0.1 + 1.0\n",
        "batch_norm_shift_1 = torch.randn((1, hidden_size)) * 0.1\n",
        "\n",
        "# batch_norm_scale_2 = torch.randn((1, hidden_size)) * 0.1 + 1.0\n",
        "# batch_norm_shift_2 = torch.randn((1, hidden_size)) * 0.1\n",
        "\n",
        "parameters = [char_emb, W1, b1, W2, b2, batch_norm_scale_1, batch_norm_shift_1]\n",
        "print(f'# of params: {sum(param.nelement() for param in parameters):,}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5h8jRP7HerdB",
        "outputId": "12be5d62-94c2-4606-a3c8-3edb93abd638"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of params: 47,070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in parameters:\n",
        "    param.requires_grad = True\n",
        "\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "n = batch_size\n",
        "lossi = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for step in range(max_steps):\n",
        "\n",
        "    # mini batch embedding\n",
        "    mini_batch = torch.randint(0, X_train.shape[0], (batch_size, ), generator=generator)\n",
        "    X_batch, y_batch = X_train[mini_batch], y_train[mini_batch] # (32, 3) (32)\n",
        "\n",
        "    # forward pass\n",
        "    emb = char_emb[X_batch] # emb: (32, 3, 10)  char_emb: (27, 10)\n",
        "    flatten_emb = emb.view(emb.shape[0], -1) # (32, 30)\n",
        "    # Linear layer 1\n",
        "    pre_batch_norm_1 = flatten_emb @ W1 + b1 # (32, 200) = (32, 30) @ (30, 200) + (200)\n",
        "    # Batch Norm 1\n",
        "    batch_norm_mean_1 = pre_batch_norm_1.mean(0, keepdim=True) # (1, 200)\n",
        "    batch_norm_var_1 = pre_batch_norm_1.var(0, keepdim=True, unbiased=True) # (1, 200)\n",
        "    batch_norm_var_inv_1 = (batch_norm_var_1 + 1e-5)**-0.5 # (1, 200)\n",
        "    batch_norm_raw_1 = (pre_batch_norm_1 - batch_norm_mean_1) * batch_norm_var_inv_1 # (32, 200) = ((32, 200) - (1, 200)) * (1, 200)\n",
        "    Z_1 = batch_norm_scale_1 * batch_norm_raw_1 + batch_norm_shift_1 # (32, 200)\n",
        "    # Activation 1\n",
        "    alpha = 0.01\n",
        "    # A_1 = np.maximum(alpha * Z_1, Z_1)\n",
        "    A_1 = torch.tanh(Z_1)\n",
        "    # Linear layer 2\n",
        "    # pre_batch_norm_2 = A_1 @ W2 + b2\n",
        "    logits = A_1 @ W2 + b2 # delete if u put back layer 3\n",
        "    # # Batch Norm 2\n",
        "    # batch_norm_mean_2 = pre_batch_norm_2.mean(0, keepdim=True)\n",
        "    # batch_norm_var_2 = pre_batch_norm_2.var(0, keepdim=True, unbiased=True)\n",
        "    # batch_norm_var_inv_2 = (batch_norm_var_2 + 1e-5)**0.5\n",
        "    # batch_norm_raw_2 = (pre_batch_norm_2 - batch_norm_mean_2) * batch_norm_var_inv_2\n",
        "    # Z_2 = batch_norm_scale_2 * batch_norm_raw_2 + batch_norm_shift_2\n",
        "    # # Activation 2\n",
        "    # # A_2 = np.maximum(alpha * Z_2, Z_2)\n",
        "    # A_2 = torch.tanh(Z_2)\n",
        "    # logits = A_2 @ W3 + b3 # (32, 27) = (32, 200) @ (200, 27) + (27)\n",
        "\n",
        "    # Loss\n",
        "    loss = F.cross_entropy(logits, y_batch)\n",
        "\n",
        "    # backward pass\n",
        "    for param in parameters:\n",
        "      param.grad = None\n",
        "    d_logits = F.softmax(logits, 1)\n",
        "    d_logits[range(n), y_batch] -= 1\n",
        "    d_logits /= n # (32, 27)\n",
        "    # Layer 3\n",
        "    # d_A_2 = d_logits @ W3.T # (32, 200) = (32, 27) @ (27, 200)\n",
        "    # d_W3 = A_2.T @ d_logits # (200, 27) = (200, 32) @ (32, 27)\n",
        "    # d_b3 = d_logits.sum(0) # (27)\n",
        "    # LeakyRelu\n",
        "    # d_Z_2 = torch.ones_like(Z_2) # (32, 200)\n",
        "    # d_Z_2[Z_2 <= 0] = alpha\n",
        "    # d_Z_2 *= d_A_2\n",
        "    # d_Z_2 = (1.0 - A_2) * d_A_2\n",
        "    # Batch norm\n",
        "    # d_batch_norm_scale_2 = (batch_norm_raw_2 * d_Z_2).sum(0, keepdim=True) # (1, 200)\n",
        "    # d_batch_norm_shift_2 = d_Z_2.sum(0, keepdim=True) # (1, 200)\n",
        "    # d_pre_batch_norm_2 = batch_norm_scale_2 * batch_norm_var_inv_2/n * (n*d_Z_2 - d_Z_2.sum(0) - n/(n-1)*batch_norm_raw_2*(d_Z_2*batch_norm_raw_2).sum(0)) # (32, 200)\n",
        "\n",
        "    # Layer 2\n",
        "    d_A = d_logits @ W2.T # (32, 200) = (32, 200) @ (200, 200)\n",
        "    d_W2 = A_1.T @ d_logits # (200, 27) = (200, 32) @ (32, 27)\n",
        "    d_b2 = d_logits.sum(0) # (27)\n",
        "    # d_A = d_Z_2 @ W2.T # (32, 200) = (32, 200) @ (200, 200)\n",
        "    # d_W2 = A_1.T @ d_Z_2 # (200, 27) = (200, 32) @ (32, 27)\n",
        "    # d_b2 = d_Z_2.sum(0) # (27)\n",
        "    # LeakyRelu\n",
        "    # d_Z = torch.ones_like(Z_1) # (32, 200)\n",
        "    # d_Z[Z_1 <= 0] = alpha\n",
        "    # d_Z *= d_A\n",
        "    d_Z = (1 - A_1) * d_A\n",
        "    # Batch norm\n",
        "    d_batch_norm_scale = (batch_norm_raw_1 * d_Z).sum(0, keepdim=True) # (1, 200)\n",
        "    d_batch_norm_shift = d_Z.sum(0, keepdim=True) # (1, 200)\n",
        "    d_pre_batch_norm = batch_norm_scale_1 * batch_norm_var_inv_1/n * (n*d_Z - d_Z.sum(0) - n/(n-1)*batch_norm_raw_1*(d_Z*batch_norm_raw_1).sum(0)) # (32, 200)\n",
        "    # Layer 1\n",
        "    d_flatten_emb = d_pre_batch_norm @ W1.T # (32, 30) = (32, 200) @ (200, 30)\n",
        "    d_W1 = flatten_emb.T @ d_pre_batch_norm # (30, 200) = (30, 32) @ (32, 200)\n",
        "    d_b1 = d_pre_batch_norm.sum(0) # (200)\n",
        "    # mini batch embedding\n",
        "    d_mini_batch = d_flatten_emb.view(emb.shape) # (32, 3, 10)\n",
        "    d_char_emb = torch.zeros_like(char_emb) # (27, 10)\n",
        "    for k in range(X_batch.shape[0]):\n",
        "      for j in range(X_batch.shape[1]):\n",
        "        idx = X_batch[k, j]\n",
        "        d_char_emb[idx] += d_mini_batch[k, j]\n",
        "\n",
        "    grads = [d_char_emb, d_W1, d_b1, d_W2, d_b2, d_batch_norm_scale, d_batch_norm_shift]\n",
        "\n",
        "    lr = 0.1 if step < 50000 else 0.01\n",
        "    for param, grad in zip(parameters, grads):\n",
        "      param.data -= lr * grad\n",
        "\n",
        "    if step % 10000 == 0:\n",
        "      print(f'({step}/{max_steps}) {loss.item():.5f}')\n",
        "\n",
        "    lossi.append(loss.log10().item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rof6Kp9d51O",
        "outputId": "9c3a30c9-2546-4ae4-d25b-329a2de6dabc"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0/200000) 5.96760\n",
            "(10000/200000) 2.41971\n",
            "(20000/200000) 2.11269\n",
            "(30000/200000) 2.08709\n",
            "(40000/200000) 2.32846\n",
            "(50000/200000) 2.21946\n",
            "(60000/200000) 2.34827\n",
            "(70000/200000) 2.28833\n",
            "(80000/200000) 2.08905\n",
            "(90000/200000) 1.98657\n",
            "(100000/200000) 2.42279\n",
            "(110000/200000) 2.50229\n",
            "(120000/200000) 2.64447\n",
            "(130000/200000) 2.55108\n",
            "(140000/200000) 2.14538\n",
            "(150000/200000) 2.17369\n",
            "(160000/200000) 2.25069\n",
            "(170000/200000) 2.18599\n",
            "(180000/200000) 1.88451\n",
            "(190000/200000) 1.87565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calibrate the batch norm at the end of training\n",
        "with torch.no_grad():\n",
        "  emb = char_emb[X_train]\n",
        "  flatten_emb = emb.view(emb.shape[0], -1)\n",
        "  Z_1 = flatten_emb @ W1 + b1\n",
        "  batch_norm_mean_1 = Z_1.mean(0, keepdim=True)\n",
        "  batch_norm_var_1 = Z_1.var(0, keepdim=True, unbiased=True)\n",
        "  # A_1 = np.maximum(alpha * Z_1, Z_1)\n",
        "  A_1 = torch.tanh(Z_1)\n",
        "  Z_2 = A_1 @ W2 + b2\n",
        "  batch_norm_mean_2 = Z_2.mean(0, keepdim=True)\n",
        "  batch_norm_var_2 = Z_2.var(0, keepdim=True, unbiased=True)"
      ],
      "metadata": {
        "id": "tjXlHzGyqsFT"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def split_loss(split):\n",
        "  X, y = {\n",
        "      'train': (X_train, y_train),\n",
        "      'valid': (X_valid, y_valid)\n",
        "  }[split]\n",
        "  emb = char_emb[X]\n",
        "  flatten_emb = emb.view(emb.shape[0], -1)\n",
        "  Z_1 = flatten_emb @ W1 + b1\n",
        "  Z_1 = batch_norm_scale_1 * (Z_1 - batch_norm_mean_1) * (batch_norm_var_1 + 1e-5)**-0.5 + batch_norm_shift_1\n",
        "  # A_1 = np.maximum(alpha*Z_1, Z_1)\n",
        "  A_1 = torch.tanh(Z_1)\n",
        "  Z_2 = A_1 @ W2 + b2\n",
        "  Z_2 = batch_norm_scale_2 * (Z_2 - batch_norm_mean_2) * (batch_norm_var_2 + 1e-5)**-0.5 + batch_norm_shift_2\n",
        "  # A_2 = np.maximum(alpha*Z_2, Z_2)\n",
        "  A_2 = torch.tanh(Z_2)\n",
        "  logits = A_2 @ W3 + b3\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('valid')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqD5DCur8NYU",
        "outputId": "f0058554-caf0-485d-c8d9-044d329f017d"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 3.6540679931640625\n",
            "valid 3.7033698558807373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = torch.Generator()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for _ in range(20):\n",
        "\n",
        "    out = []\n",
        "    context = [0] * context_size\n",
        "    while True:\n",
        "      # forward pass\n",
        "      emb = char_emb[torch.tensor([context])]\n",
        "      flatten_emb = emb.view(emb.shape[0], -1)\n",
        "      Z_1 = flatten_emb @ W1 + b1\n",
        "      Z_1 = batch_norm_scale_1 * (Z_1 - batch_norm_mean_1) * (batch_norm_var_1 + 1e-5)**-0.5 + batch_norm_shift_1\n",
        "      # A_1 = np.maximum(alpha*Z_1, Z_1)\n",
        "      A_1 = torch.tanh(Z_1)\n",
        "      logits = A_1 @ W2 + b2 #Z_2\n",
        "      # Z_2 = batch_norm_scale_2 * (Z_2 - batch_norm_mean_2) * (batch_norm_var_2 + 1e-5)**-0.5 + batch_norm_shift_2\n",
        "      # A_2 = np.maximum(alpha*Z_2, Z_2)\n",
        "      # A_2 = torch.tanh(Z_2)\n",
        "      # logits = A_2 @ W3 + b3\n",
        "\n",
        "      # sample\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      idx = torch.multinomial(probs, num_samples=1, generator=generator).item()\n",
        "      context = context[1:] + [idx]\n",
        "      out.append(idx)\n",
        "      if idx == 0:\n",
        "        break\n",
        "\n",
        "    print(''.join(idx_to_char[i] for i in out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odROJTya9At-",
        "outputId": "7e61dff3-a026-460d-edbb-8747e2d90383"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cyniatelfinghyn.\n",
            "yla.\n",
            "tuanaaie.\n",
            "taimrayie.\n",
            "maiel.\n",
            "lilani.\n",
            "aadlon.\n",
            "marber.\n",
            "liannigrachi.\n",
            "sanibrigh.\n",
            "keiancy.\n",
            "evyanisly.\n",
            "morie.\n",
            "lies.\n",
            "aniva.\n",
            "cata.\n",
            "charous.\n",
            "naviannyus.\n",
            "cau.\n",
            "marion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pir4chi7Bmt2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}